{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde6ba17-2b35-4dc7-8888-e72602a77416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 241 dump files in C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1.\n",
      "Warning: C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1\\slm_heat_transfer_data.csv not found. Creating a placeholder dataset.\n",
      "Training LightGBM ...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1007\n",
      "[LightGBM] [Info] Number of data points in the train set: 964000, number of used features: 4\n",
      "[LightGBM] [Info] Start training from score 0.000795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAY02\\anaconda3\\envs\\SLM_Python\\lib\\site-packages\\lightgbm\\basic.py:374: UserWarning: Converting column-vector to 1d array\n",
      "  _log_warning(\"Converting column-vector to 1d array\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.059695253633636924\n",
      "LightGBM Model trained on multiple timesteps and saved in the specified folder.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Define the path to the dump files folder\n",
    "dump_folder = r\"C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1\"\n",
    "\n",
    "# Function to process multiple dump files\n",
    "def process_dump_files(dump_files):\n",
    "    all_data = []\n",
    "    for file in dump_files:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Find the relevant section with atom data\n",
    "        start_reading = False\n",
    "        atom_data = []\n",
    "        for line in lines:\n",
    "            if \"ITEM: ATOMS\" in line:\n",
    "                start_reading = True\n",
    "                headers = line.strip().split()[2:]\n",
    "                continue\n",
    "            if start_reading:\n",
    "                atom_data.append(line.strip().split())\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(atom_data, columns=headers)\n",
    "        df = df.astype(float)\n",
    "        \n",
    "        # Add timestep column (extracted from filename or content)\n",
    "        timestep = int(file.split(\"coord\")[-1].split(\".dump\")[0])\n",
    "        df[\"timestep\"] = timestep\n",
    "        \n",
    "        all_data.append(df)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Get all dump files\n",
    "dump_files = glob.glob(os.path.join(dump_folder, \"coord*.dump\"))\n",
    "print(f\"Found {len(dump_files)} dump files in {dump_folder}.\")\n",
    "\n",
    "# Process all dump files\n",
    "df_atoms = process_dump_files(dump_files)\n",
    "\n",
    "# Check if the external simulation parameters file exists\n",
    "params_file = os.path.join(dump_folder, \"slm_heat_transfer_data.csv\")\n",
    "if os.path.exists(params_file):\n",
    "    data_params = pd.read_csv(params_file)\n",
    "else:\n",
    "    print(f\"Warning: {params_file} not found. Creating a placeholder dataset.\")\n",
    "    data_params = pd.DataFrame({\n",
    "        \"laser_power\": [100] * len(df_atoms),\n",
    "        \"scan_speed\": [10] * len(df_atoms),\n",
    "        \"layer_thickness\": [0.05] * len(df_atoms),\n",
    "        \"powder_density\": [8000] * len(df_atoms),\n",
    "    })\n",
    "\n",
    "# Merge with extracted atom data\n",
    "data = pd.concat([df_atoms, data_params], axis=1)\n",
    "\n",
    "# Define input and output columns\n",
    "input_features = [\"laser_power\", \"scan_speed\", \"layer_thickness\", \"powder_density\", \"x\", \"y\", \"z\", \"timestep\"]\n",
    "target_feature = \"f_Temp[0]\"\n",
    "\n",
    "# Extract inputs and outputs\n",
    "X = data[input_features].values\n",
    "y = data[target_feature].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM Regressor\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31\n",
    "}\n",
    "\n",
    "print(\"Training LightGBM ...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[lgb_train, lgb_test],\n",
    "    valid_names=[\"train\",\"valid\"],\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, os.path.join(dump_folder, \"heat_transfer_lgbm_model.pkl\"))\n",
    "print(\"LightGBM Model trained on multiple timesteps and saved in the specified folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee58830-8bec-4fbd-9dbe-8c798ae11f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Temperature: 278.16456624750396\n"
     ]
    }
   ],
   "source": [
    "# Example: New test input (modify these values as needed)\n",
    "new_data = np.array([[100, 10, 0.05, 8000, 0.002, -0.013, 0.001, 1100000]])  # Replace with real values\n",
    "\n",
    "# Apply the same scaling used in training\n",
    "new_data_scaled = scaler_X.transform(new_data)  # Use the same scaler as training\n",
    "\n",
    "\n",
    "# Predict temperature for the new sample\n",
    "predicted_temperature = model.predict(new_data_scaled)\n",
    "\n",
    "# Convert back to the original scale (if used)\n",
    "predicted_temperature = scaler_y.inverse_transform(predicted_temperature.reshape(-1, 1))\n",
    "\n",
    "print(f\"Predicted Temperature: {predicted_temperature[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8185a36d-4b7d-43c5-9f34-72055f26178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Found 241 dump files in C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1.\n",
      "Warning: C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1\\slm_heat_transfer_data.csv not found. Creating a placeholder dataset.\n",
      "Predicted temperatures saved to C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1\\predicted_temperatures.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Define the path to the dump files folder\n",
    "dump_folder = r\"C:\\Users\\MAY02\\Desktop\\SLM_NeuralNetwork\\Relax_Al_helix_1_1\"\n",
    "\n",
    "# Load trained model\n",
    "model_path = os.path.join(dump_folder, \"heat_transfer_lgbm_model.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# Function to process multiple dump files\n",
    "def process_dump_files(dump_files):\n",
    "    all_data = []\n",
    "    for file in dump_files:\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Find the relevant section with atom data\n",
    "        start_reading = False\n",
    "        atom_data = []\n",
    "        for line in lines:\n",
    "            if \"ITEM: ATOMS\" in line:\n",
    "                start_reading = True\n",
    "                headers = line.strip().split()[2:]\n",
    "                continue\n",
    "            if start_reading:\n",
    "                atom_data.append(line.strip().split())\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(atom_data, columns=headers)\n",
    "        df = df.astype(float)\n",
    "        \n",
    "        # Add timestep column (extracted from filename or content)\n",
    "        timestep = int(file.split(\"coord\")[-1].split(\".dump\")[0])\n",
    "        df[\"timestep\"] = timestep\n",
    "        \n",
    "        all_data.append(df)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Get all dump files\n",
    "dump_files = glob.glob(os.path.join(dump_folder, \"coord*.dump\"))\n",
    "print(f\"Found {len(dump_files)} dump files in {dump_folder}.\")\n",
    "\n",
    "# Process all dump files\n",
    "df_atoms = process_dump_files(dump_files)\n",
    "\n",
    "# Check if the external simulation parameters file exists\n",
    "params_file = os.path.join(dump_folder, \"slm_heat_transfer_data.csv\")\n",
    "if os.path.exists(params_file):\n",
    "    data_params = pd.read_csv(params_file)\n",
    "else:\n",
    "    print(f\"Warning: {params_file} not found. Creating a placeholder dataset.\")\n",
    "    data_params = pd.DataFrame({\n",
    "        \"laser_power\": [100] * len(df_atoms),\n",
    "        \"scan_speed\": [10] * len(df_atoms),\n",
    "        \"layer_thickness\": [0.05] * len(df_atoms),\n",
    "        \"powder_density\": [8000] * len(df_atoms),\n",
    "    })\n",
    "\n",
    "# Merge with extracted atom data\n",
    "data = pd.concat([df_atoms, data_params], axis=1)\n",
    "\n",
    "# Define input features for prediction\n",
    "input_features = [\"laser_power\", \"scan_speed\", \"layer_thickness\", \"powder_density\", \"x\", \"y\", \"z\", \"timestep\"]\n",
    "X_new = data[input_features].values[:5000]  # Select 5000 atoms for prediction\n",
    "\n",
    "# Normalize using previously fitted scalers\n",
    "scaler_X = StandardScaler()\n",
    "X_new_scaled = scaler_X.fit_transform(X_new)\n",
    "\n",
    "# Predict temperatures\n",
    "y_pred = model.predict(X_new_scaled)\n",
    "\n",
    "# Output predictions\n",
    "predictions_df = data.iloc[:5000].copy()\n",
    "predictions_df[\"Predicted_Temperature\"] = y_pred\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "predictions_file = os.path.join(dump_folder, \"predicted_temperatures.csv\")\n",
    "predictions_df.to_csv(predictions_file, index=False)\n",
    "print(f\"Predicted temperatures saved to {predictions_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
